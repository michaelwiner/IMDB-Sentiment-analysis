{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten , LSTM , Embedding\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words = max_features , index_from= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "divider = (len(x_train)// batch_size)*batch_size\n",
    "x_train = x_train[0:divider] \n",
    "y_train = y_train[0:divider] \n",
    "x_test = x_test[0:divider] \n",
    "y_test = y_test[0:divider] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 24992 samples, validate on 24992 samples\n",
      "Epoch 1/15\n",
      "24992/24992 [==============================] - 168s 7ms/step - loss: 2.0137 - acc: 0.7166 - val_loss: 0.4620 - val_acc: 0.8167\n",
      "Epoch 2/15\n",
      "24992/24992 [==============================] - 178s 7ms/step - loss: 1.8630 - acc: 0.7984 - val_loss: 0.4450 - val_acc: 0.8134\n",
      "Epoch 3/15\n",
      "24992/24992 [==============================] - 244s 10ms/step - loss: 1.7885 - acc: 0.8285 - val_loss: 0.4608 - val_acc: 0.8144\n",
      "Epoch 4/15\n",
      "20000/24992 [=======================>......] - ETA: 29s - loss: 1.7541 - acc: 0.8482"
     ]
    }
   ],
   "source": [
    "\n",
    "# Expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# Note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features ,256 , batch_input_shape= (batch_size, 80 ) ))\n",
    "model.add(LSTM(64, dropout =0.2 , stateful=True  , return_sequences= False  ))\n",
    "model.add(Dense(1  , activation = 'sigmoid'   ))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "earlystop = EarlyStopping( monitor= 'val_loss' , patience= 2 )\n",
    "    \n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=False,\n",
    "          epochs=15 ,\n",
    "          callbacks = [earlystop] ,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size )\n",
    "\n",
    "\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.62964791],\n",
       "       [ 0.00951354],\n",
       "       [ 0.02527248],\n",
       "       [ 0.18654539],\n",
       "       [ 0.02219991]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train , batch_size=batch_size)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while we enter their world and feel every second with them there is so much beyond the climb that makes everything they go through so much more tense touching the void was also a great doc about mountain climbing and showing the intensity in an engaging way but this film is much more of a human story i just saw it today but i will go and say that this is one of the best documentaries i have ever seen\n"
     ]
    }
   ],
   "source": [
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in x_train[8]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
